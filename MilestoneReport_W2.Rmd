---
title: "Milestone Report - Capstone Project"
author: "Luciano Guerra"
date: "May 15, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Milestone Report - Capstone project  
### Summary 

The goal of this project is just to display that you've gotten used to working with the data and that you are on track to create your prediction algorithm. Please submit a report on R Pubs (http://rpubs.com/) that explains your exploratory analysis and your goals for the eventual app and algorithm. This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. You should make use of tables and plots to illustrate important summaries of the data set.   

The motivation for this project is to:   
1. Demonstrate that you've downloaded the data and have successfully loaded it in.  
2. Create a basic report of summary statistics about the data sets.  
3. Report any interesting findings that you amassed so far.  
4. Get feedback on your plans for creating a prediction algorithm and Shiny app.  
     
### Acquiring data  
We must create a predictive model based on language data from HC Corpora. The HC Corpora is conformed by twelve corpora divided in four languages (english, russian, finish and german). Every language has a twitter, blogs and news corpus (the prediction model will only focus on the english data).  

I shall use exclusively the english version of these files.

We shall download files if needed and unzip them into the correct folder. After that summaryze info in words, lines and disk space.
```{r acquiring_data, echo = TRUE}

    #load the libraries
    totalPack <- installed.packages()

    if(!("stringi" %in% totalPack)){
        install.packages("stringi")
    }

    library(stringi)

    connection <- file("final/en_US/en_US.news.txt", open = "r")
    newLines <- readLines(con = connection, encoding="UTF-8", skipNul = TRUE, warn = FALSE, ok = TRUE)
    close(connection)
    connection <- file("final/en_US/en_US.twitter.txt", open = "r")
    twitterLines <- readLines(con = connection, encoding="UTF-8", skipNul = TRUE, warn = FALSE, ok = TRUE)
    close(connection)
    connection <- file("final/en_US/en_US.blogs.txt", open = "r")
    blogsLines <- readLines(con = connection, encoding="UTF-8", skipNul = TRUE, warn = FALSE, ok = TRUE)
    close(connection)
    
    # Step 3: Summarize the data
    ## Get files sizes
    newLines.size <- file.size("final/en_US/en_US.news.txt")
    twitterLines.size <- file.size("final/en_US/en_US.twitter.txt")
    blogsLines.size <- file.size("final/en_US/en_US.blogs.txt")
    
    ## Get words in flies
    newLines.words <- stri_count_words(str = newLines, locale = "")
    twitterLines.words <- stri_count_words(str = twitterLines, locale = "")
    blogsLines.words <- stri_count_words(str = blogsLines, locale = "")
    
    ## Summary of data sets
    usDataSumm <- data.frame(source = c("news", "twitter", "blogs"),
    file.size.MB = c(newLines.size, twitterLines.size, blogsLines.size),
    num.lines = c(length(newLines), length(twitterLines), length(blogsLines)),
    mean.num.words = c(mean(newLines.words), mean(twitterLines.words), mean(twitterLines.words)))
    print(usDataSumm)
```

### Tyding data
Datasets read from the files must be cleaned and prepared for tokneization

```{r tyding_data, echo = TRUE}
    ## Install needed libraries
    if(!("tm" %in% totalPack)){
        install.packages("tm")
    }
    if(!("RWeka" %in% totalPack)){
        install.packages("RWeka")
    }
    library(RWeka)
    library(tm)
    sampNum <- 2000 
    sampleNewSel <- sample(1:length(newLines), size = sampNum, replace = F)
    sampleTwitterSel <- sample(1:length(twitterLines), size = sampNum, replace = F)
    sampleBlogSel <- sample(1:length(blogsLines), size = sampNum, replace = F)
    
    sampleNew <- newLines[sampleNewSel]
    sampleTwitter <- twitterLines[sampleTwitterSel]
    sampleBlogs <- blogsLines[sampleBlogSel]
    
    sampleNew <- iconv(x = sampleNew, "latin1", "ASCII", sub = " ")
    sampleNew <- gsub("[^[:alpha:][:space:][:punct:]]", "", sampleNew)
    sampleTwitter <- iconv(x = sampleTwitter, "latin1", "ASCII", sub = " ")
    sampleTwitter <- gsub("[^[:alpha:][:space:][:punct:]]", "", sampleTwitter)
    sampleBlogs <- iconv(x = sampleBlogs, "latin1", "ASCII", sub = " ")
    sampleBlogs <- gsub("[^[:alpha:][:space:][:punct:]]", "", sampleBlogs)
    
    corpSNew <- VCorpus(VectorSource(sampleNew))
    corpSTwitt <- VCorpus(VectorSource(sampleTwitter))
    corpSBlogs <- VCorpus(VectorSource(sampleBlogs))
    
    corp <- c(corpSNew, corpSTwitt, corpSBlogs)
    remove(corpSNew, corpSTwitt, corpSBlogs)
    remove(sampleNew, sampleTwitter, sampleBlogs)
    
    ## Tyding data
    corp <- tm_map(corp, removeNumbers)
    corp <- tm_map(corp, removePunctuation, preserve_intra_word_dashes = TRUE)
    corp <- tm_map(corp, stripWhitespace)
    corp <- tm_map(corp, content_transformer(tolower))
    corp <- tm_map(corp, removeWords, stopwords("en"))
    corp <- tm_map(corp, removeWords, "[!\\#$%&'*+,./)(:;<=>?@\\][\\^`{|}~]")
    badWords <- VectorSource(readLines("./final/BadWords.txt"))
    corp <- tm_map(corp, removeWords, badWords)
    corp <- tm_map(corp, PlainTextDocument)

```

### Tokenization
Acording with the frecuency of each word we can use tokenization to divide in diferent N-Grams. An N-gram is essentially a group of words that appear in order, with the N value representing how many words are used.

```{r tokenization, echo = TRUE}

UniToken<-function(x)NGramTokenizer(x, Weka_control(min = 1, max = 1))
BiToken<-function(x)NGramTokenizer(x, Weka_control(min = 2, max = 2))
TriToken<-function(x)NGramTokenizer(x, Weka_control(min = 3, max = 3))
QuadToken <- function(x)(NGramTokenizer(x, Weka_control(min=4, max = 4)))

unigrams<-function(x){
        tdm <- TermDocumentMatrix(x, control = list(tokenize = UniToken))
        fm <- rowSums(as.matrix(tdm))
        ngram<-data.frame(ngram=names(fm),freq=fm)
        ngram<-ngram[order(-ngram$freq),]
}
bigrams<-function(x){
        tdm <- TermDocumentMatrix(x, control = list(tokenize = BiToken))
        fm <- rowSums(as.matrix(tdm))
        ngram<-data.frame(ngram=names(fm),freq=fm)
        ngram<-ngram[order(-ngram$freq),]
}
trigrams<-function(x){
        tdm <- TermDocumentMatrix(x, control = list(tokenize = TriToken))
        fm <- rowSums(as.matrix(tdm))
        ngram<-data.frame(ngram=names(fm),freq=fm)
        ngram<-ngram[order(-ngram$freq),]
}
quadgrams<-function(x){
        tdm <- TermDocumentMatrix(x, control = list(tokenize = QuadToken))
        fm <- rowSums(as.matrix(tdm))
        ngram<-data.frame(ngram=names(fm),freq=fm)
        ngram<-ngram[order(-ngram$freq),]
}

Unigrams <- unigrams(corp)
Bigrams <- bigrams(corp)
Trigrams <- trigrams(corp)
Quadgram <- quadgrams(corp)
```
Once we have tokenized our data we can use the TTR (Type/Token Ratio) which is the total word typed divided by tokens.

TTR indicates complexity, it means the more number of type words in front of same number of tokens, the more complex the vocabulary is.

### Exploratory Analysis
Afeter tokenized our data, we can see it in different graphs.

Like wordcloud.
```{r cloudgraph, warning=FALSE}
if(!("wordcloud" %in% totalPack)){
    install.packages("wordcloud")
}
library(wordcloud)

wordcloud(Unigrams$ngram, Unigrams$freq, scale=c(3,0.5), max.words=100, random.order=TRUE, 
          rot.per=0.5, use.r.layout=FALSE, colors=brewer.pal(8,"Dark2"))
wordcloud(Bigrams$ngram, Bigrams$freq, scale=c(2,0.5), max.words=100, random.order=TRUE, 
              rot.per=0.5, use.r.layout=FALSE, colors=brewer.pal(8,"Dark2"))
wordcloud(Trigrams$ngram, Trigrams$freq, scale=c(2,0.5), max.words=100, random.order=TRUE, 
          rot.per=0.5, use.r.layout=FALSE, colors=brewer.pal(8,"Dark2"))
wordcloud(Quadgram$ngram, Quadgram$freq, scale=c(1,0.2), max.words=100, random.order=TRUE, 
          rot.per=0.5, use.r.layout=FALSE, colors=brewer.pal(8,"Dark2"))

```

#### Most frequent words
```{r wordgraph, warning=FALSE}
if(!("ggplot2" %in% totalPack)){
    install.packages("ggplot2")
}
library(ggplot2)
    unigramDF <- data.frame(w = Unigrams$ngram[1:20], f = Unigrams$freq[1:20])
    uniplot <- ggplot(unigramDF, aes(x = reorder(w, f), y = f, fill = factor(reorder(w, -f)))) +
        geom_bar(stat = "identity") + theme(legend.title=element_blank(), legend.position = "none") + 
           xlab("Word") + ylab("Frecuency") + coord_flip()
    uniplot
     
```

#### Frecuencies of most popular 2-grams
```{r 2gramsgraph, warning=FALSE}
    bigramDF <- data.frame(w = Bigrams$ngram[1:20], f = Bigrams$freq[1:20])
        biplot <- ggplot(bigramDF, aes(x = reorder(w, f), y = f, fill = factor(reorder(w, -f)))) +
            geom_bar(stat = "identity") + theme(legend.title=element_blank(), legend.position = "none") + 
               xlab("Word") + ylab("Frecuency")  + coord_flip()
        biplot
```

#### Frecuencies of most popular 3-grams
```{r 3grampsgraph, warning=FALSE}
trigramDF <- data.frame(w = Trigrams$ngram[1:20], f = Trigrams$freq[1:20])
    triplot <- ggplot(trigramDF, aes(x = reorder(w, f), y = f, fill = factor(reorder(w, -f)))) +
        geom_bar(stat = "identity") + theme(legend.title=element_blank(), legend.position = "none") + 
           xlab("Word") + ylab("Frecuency") + coord_flip()
    triplot
```

#### Frecuencies of most popular 4-grams
```{r 4grampsgraph, warning=FALSE}
quadgramDF <- data.frame(w = Quadgram$ngram[1:20], f = Quadgram$freq[1:20])
    quadplot <- ggplot(quadgramDF, aes(x = reorder(w, f), y = f, fill = factor(reorder(w, -f)))) +
        geom_bar(stat = "identity") + theme(legend.title=element_blank(), legend.position = "none") + 
           xlab("Word") + ylab("Frecuency") + coord_flip()
    quadplot
```

## Next steps
As already noted, the next step of the capstone project will be to create a prediction application.   
To create a smooth and fast application it is absolutely necessary to build a fast prediction algorithm.   
We need to find ways for a faster processing of larger datasets. Next to that, increasing the value of n for n-gram tokenization will improve the prediction accuracy.   
All in all a shiny application will be created which will be able to predict the next word a user wants to write.  